import { OpenAI } from "openai"
import { GoogleGenerativeAI } from "@google/generative-ai" // For user fallback
import { NextResponse } from "next/server"
import { SYSTEM_CONTEXT } from "@/lib/system-context"
import { formatContentKnowledgeForGemini } from "@/lib/content-knowledge-base"

interface Message {
  role: "user" | "assistant"
  content: string
}

// ========================================
// RATE LIMITING CONFIGURATION
// ========================================
// 10 requests per session per 120 minutes
const RATE_LIMIT = 10
const RATE_LIMIT_WINDOW = 120 * 60 * 1000 // 120 minutes in milliseconds

// In-memory storage for rate limiting (per session)
// In production, consider using Redis or similar
const rateLimitStore = new Map<string, { count: number; resetTime: number }>()

function checkRateLimit(sessionId: string): { allowed: boolean; remaining: number; resetTime: number } {
  const now = Date.now()
  const userLimit = rateLimitStore.get(sessionId)

  if (!userLimit || now > userLimit.resetTime) {
    // New session or expired window
    const resetTime = now + RATE_LIMIT_WINDOW
    rateLimitStore.set(sessionId, { count: 1, resetTime })
    return { allowed: true, remaining: RATE_LIMIT - 1, resetTime }
  }

  if (userLimit.count >= RATE_LIMIT) {
    // Rate limit exceeded
    return { allowed: false, remaining: 0, resetTime: userLimit.resetTime }
  }

  // Increment count
  userLimit.count++
  return { allowed: true, remaining: RATE_LIMIT - userLimit.count, resetTime: userLimit.resetTime }
}

// ========================================
// OPENAI CLIENT (PRIMARY - DEFAULT)
// ========================================
function getOpenAIClient() {
  const apiKey = process.env.OPENAI_API_KEY
  
  if (!apiKey || apiKey === 'INSERT_YOUR_OPENAI_API_KEY_HERE') {
    console.error(`‚ùå OpenAI API key not configured (OPENAI_API_KEY)`)
    return null
  }
  
  console.log(`‚úÖ OpenAI API key configured (${apiKey.substring(0, 8)}...${apiKey.substring(apiKey.length - 4)})`)
  return new OpenAI({ apiKey })
}

// ========================================
// TEMPORARILY COMMENTED OUT - GEMINI CLIENTS
// ========================================
/*
// Get Gemini client (primary or fallback)
function getGeminiClient(useFallback: boolean = false) {
  const apiKey = useFallback ? process.env.GEMINI_API_KEY_2 : process.env.GEMINI_API_KEY
  const keyType = useFallback ? 'FALLBACK' : 'PRIMARY'
  const envVar = useFallback ? 'GEMINI_API_KEY_2' : 'GEMINI_API_KEY'
  
  if (!apiKey || apiKey === 'INSERT_YOUR_SECOND_GEMINI_API_KEY_HERE' || apiKey === 'INSERT_YOUR_GEMINI_API_KEY_HERE') {
    console.error(`‚ùå ${keyType} Gemini API key not configured (${envVar})`)
    return null
  }
  
  console.log(`‚úÖ ${keyType} Gemini API key configured (${envVar}: ${apiKey.substring(0, 8)}...${apiKey.substring(apiKey.length - 4)})`)
  const genAI = new GoogleGenerativeAI(apiKey)
  return genAI.getGenerativeModel({ model: "gemini-2.0-flash-exp" })
}
*/

// Helper to check if error is rate limit
function isRateLimitError(error: any): boolean {
  if (!error) {
    console.log('üîç isRateLimitError: No error object')
    return false
  }
  
  const errorString = JSON.stringify(error).toLowerCase()
  const message = error?.message?.toLowerCase() || ''
  const errorResponse = error?.response?.data?.error?.message?.toLowerCase() || ''
  
  const isRateLimit = (
    error?.status === 429 ||
    error?.statusCode === 429 ||
    error?.response?.status === 429 ||
    message.includes('429') ||
    message.includes('rate limit') ||
    message.includes('too many requests') ||
    message.includes('quota') ||
    message.includes('resource exhausted') ||
    errorString.includes('429') ||
    errorString.includes('resource_exhausted') ||
    errorString.includes('rate limit') ||
    errorResponse.includes('quota') ||
    errorResponse.includes('rate limit')
  )
  
  console.log(`üîç isRateLimitError result: ${isRateLimit}`)
  console.log(`   Status: ${error?.status || error?.statusCode || error?.response?.status}`)
  console.log(`   Message snippet: ${message.substring(0, 100)}`)
  
  return isRateLimit
}

export async function POST(request: Request) {
  try {
    const { prompt, context, conversationHistory = [], mode = "tutor", query, userApiKey, sessionId = "default" } = await request.json()

    // Check rate limit
    const rateLimitResult = checkRateLimit(sessionId)
    if (!rateLimitResult.allowed) {
      const minutesRemaining = Math.ceil((rateLimitResult.resetTime - Date.now()) / 60000)
      console.log(`‚è±Ô∏è Rate limit exceeded for session ${sessionId}`)
      return NextResponse.json({
        error: `Has alcanzado el l√≠mite de ${RATE_LIMIT} consultas. Por favor, esper√° ${minutesRemaining} minutos antes de hacer otra consulta.`,
        success: false,
        errorType: 'rate_limit',
        remaining: 0,
        resetTime: rateLimitResult.resetTime
      }, { status: 429 })
    }

    console.log(`‚úÖ Rate limit check passed. Remaining: ${rateLimitResult.remaining}/${RATE_LIMIT}`)

    // Handle Br√∫jula mode early (uses 'query' instead of 'prompt')
    if (mode === "brujula") {
      console.log("üß≠ [BR√öJULA MODE] Starting request")
      console.log(`üìù Query: "${query?.substring(0, 100)}${query?.length > 100 ? '...' : ''}"`)
      
      // If user provided their own API key, use Gemini with user's key (easier to get)
      if (userApiKey && userApiKey.trim()) {
        console.log("üîë [BR√öJULA] User provided their own Gemini API key, using it directly")
        try {
          const genAI = new GoogleGenerativeAI(userApiKey.trim())
          const userModel = genAI.getGenerativeModel({ model: "gemini-2.0-flash-exp" })
          const result = await handleBrujulaMode(query, userModel, "user-provided-gemini", false, true)
          console.log(`‚úÖ [BR√öJULA] User Gemini API key success`)
          return result
        } catch (userKeyError: any) {
          console.error("‚ùå [BR√öJULA] User API key failed:", userKeyError?.message)
          // If user's key fails, return specific error
          return NextResponse.json({
            error: "La API key de Gemini que proporcionaste no es v√°lida o ha alcanzado su l√≠mite. Verific√° que sea correcta.",
            success: false,
            errorType: 'invalid_user_key'
          }, { status: 400 })
        }
      }
      
      const openaiClient = getOpenAIClient()
      
      // If OpenAI is not configured, return error
      if (!openaiClient) {
        console.error("‚ùå [BR√öJULA] No API keys configured")
        return NextResponse.json(
          { error: "No hay ning√∫n proveedor de IA configurado. Por favor, configur√° OPENAI_API_KEY en el archivo .env.local", success: false },
          { status: 500 }
        )
      }
      
      // Use OpenAI for Br√∫jula
      const startTime = Date.now()
      try {
        console.log("üöÄ [BR√öJULA] Attempting OpenAI API...")
        const result = await handleBrujulaMode(query, openaiClient, "openai", false)
        const duration = Date.now() - startTime
        console.log(`‚úÖ [BR√öJULA] OpenAI API success (${duration}ms)`)
        
        // Add rate limit info to response
        const response = await result.json()
        return NextResponse.json({
          ...response,
          rateLimit: {
            remaining: rateLimitResult.remaining,
            total: RATE_LIMIT,
            resetTime: rateLimitResult.resetTime
          }
        })
      } catch (error: any) {
        const duration = Date.now() - startTime
        console.log(`‚ö†Ô∏è [BR√öJULA] OpenAI API failed (${duration}ms)`)
        console.error("Error details:", {
          status: error?.status,
          statusCode: error?.statusCode,
          message: error?.message,
          type: error?.constructor?.name
        })
        
        // Check if it's a rate limit error
        if (isRateLimitError(error)) {
          console.log("üîÑ [BR√öJULA] Rate limit detected on OpenAI")
          return NextResponse.json({
            error: "El servicio de b√∫squeda est√° temporalmente sobrecargado. Por favor, intent√° de nuevo en unos segundos o us√° tu propia API key.",
            success: false,
            errorType: 'rate_limit'
          }, { status: 429 })
        }
        // Not a rate limit error, re-throw
        throw error
      }
    }

    // For Tutor mode, prompt is required
    if (!prompt) {
      return NextResponse.json(
        { error: "El prompt es requerido" },
        { status: 400 }
      )
    }

    console.log("üéì [TUTOR MODE] Starting request")
    console.log(`üìù Prompt: "${prompt?.substring(0, 100)}${prompt?.length > 100 ? '...' : ''}"`)
    console.log(`üìö Context length: ${context?.length || 0} chars`)
    console.log(`üí¨ Conversation history: ${conversationHistory?.length || 0} messages`)
    
    // If user provided their own API key, use Gemini with user's key (easier to get)
    if (userApiKey && userApiKey.trim()) {
      console.log("üîë [TUTOR] User provided their own Gemini API key, using it directly")
      try {
        const genAI = new GoogleGenerativeAI(userApiKey.trim())
        const userModel = genAI.getGenerativeModel({ model: "gemini-2.0-flash-exp" })
        const result = await handleTutorMode(userModel, prompt, context, conversationHistory, "user-provided-gemini", false, true)
        console.log(`‚úÖ [TUTOR] User Gemini API key success`)
        return result
      } catch (userKeyError: any) {
        console.error("‚ùå [TUTOR] User API key failed:", userKeyError?.message)
        return NextResponse.json({
          error: "La API key de Gemini que proporcionaste no es v√°lida o ha alcanzado su l√≠mite. Verific√° que sea correcta.",
          success: false,
          errorType: 'invalid_user_key'
        }, { status: 400 })
      }
    }
    
    const openaiClient = getOpenAIClient()
    
    // If OpenAI is not configured, return error
    if (!openaiClient) {
      console.error("‚ùå [TUTOR] No API keys configured")
      return NextResponse.json(
        { error: "No hay ning√∫n proveedor de IA configurado. Por favor, configur√° OPENAI_API_KEY en el archivo .env.local", success: false },
        { status: 500 }
      )
    }

    // Use OpenAI for Tutor mode
    const startTime = Date.now()
    try {
      console.log("üöÄ [TUTOR] Attempting OpenAI API...")
      const result = await handleTutorMode(openaiClient, prompt, context, conversationHistory, "openai", false)
      const duration = Date.now() - startTime
      console.log(`‚úÖ [TUTOR] OpenAI API success (${duration}ms)`)
      
      // Add rate limit info to response
      const response = await result.json()
      return NextResponse.json({
        ...response,
        rateLimit: {
          remaining: rateLimitResult.remaining,
          total: RATE_LIMIT,
          resetTime: rateLimitResult.resetTime
        }
      })
    } catch (error: any) {
      const duration = Date.now() - startTime
      console.log(`‚ö†Ô∏è [TUTOR] OpenAI API failed (${duration}ms)`)
      console.error("Error details:", {
        status: error?.status,
        statusCode: error?.statusCode,
        message: error?.message,
        type: error?.constructor?.name
      })
      
      // Check if it's a rate limit error
      if (isRateLimitError(error)) {
        console.log("üîÑ [TUTOR] Rate limit detected on OpenAI")
        return NextResponse.json({
          error: "El servicio de IA est√° temporalmente sobrecargado. Por favor, intent√° de nuevo en unos segundos o us√° tu propia API key.",
          success: false,
          errorType: 'rate_limit'
        }, { status: 429 })
      }
      // Not a rate limit error, re-throw
      throw error
    }

  } catch (error) {
    console.error("Error en API:", error)
    
    // Proporcionar m√°s detalles del error
    let errorMessage = "Error al procesar la solicitud"
    if (error instanceof Error) {
      errorMessage = error.message
      console.error("Detalles del error:", {
        message: error.message,
        stack: error.stack,
        name: error.name
      })
    }
    
    return NextResponse.json(
      { 
        error: errorMessage,
        success: false 
      },
      { status: 500 }
    )
  }
}

// Handle Tutor mode with OpenAI or Gemini
async function handleTutorMode(
  client: any, // OpenAI or Gemini model
  prompt: string,
  context: string,
  conversationHistory: Message[],
  provider: string,
  usedFallback: boolean = false,
  isGemini: boolean = false
) {
  const apiType = usedFallback ? 'FALLBACK' : 'PRIMARY'
  console.log(`‚öôÔ∏è  [TUTOR] Processing with ${apiType} API (${provider})`)
  
  // Use system context
  const systemContext = `CONTEXTO DEL SISTEMA:

${SYSTEM_CONTEXT}

---

INSTRUCCIONES ADICIONALES:
Cuando el usuario pregunte sobre "flujo de vibecoding" o "integrar en vibecoding", refi√©rete siempre al proceso de programar con asistencia de IA usando las herramientas mencionadas (Cursor, Claude, v0, Lovable, etc.), NO a usar herramientas no-code/low-code.`

  // Build conversation context
  let conversationContext = ""
  if (conversationHistory.length > 0) {
    conversationContext = "\n\nHISTORIAL DE LA CONVERSACI√ìN:\n"
    conversationHistory.forEach((msg: Message) => {
      conversationContext += `\n${msg.role === "user" ? "Usuario" : "Asistente"}: ${msg.content}\n`
    })
    conversationContext += "\n---\n"
  }

  // Check if context is minimal
  const isMinimalContext = context && context.trim().length < 100

  const fullPrompt = context
    ? `${systemContext}

---

${isMinimalContext 
  ? `T√©rmino del glosario de desarrollo: ${context.replace(/\*\*/g, "").trim()}`
  : `Contexto del glosario de desarrollo:\n${context}`}
${conversationContext}

Solicitud actual del usuario: ${prompt}

---

INSTRUCCIONES DE RESPUESTA:
Responde de manera clara y did√°ctica, usando lenguaje accesible para personas que est√°n aprendiendo a programar con IA (vibecoding). Si es relevante, incluye ejemplos pr√°cticos y explica c√≥mo aplicar el concepto en un flujo de desarrollo asistido por IA.

${isMinimalContext ? "NOTA: El usuario est√° preguntando sobre un t√©rmino espec√≠fico del glosario. Proporciona una explicaci√≥n completa bas√°ndote en tu conocimiento del t√©rmino." : ""}

${conversationHistory.length > 0 ? "IMPORTANTE: Tienes contexto de la conversaci√≥n anterior. Usa esa informaci√≥n para dar respuestas m√°s espec√≠ficas y relevantes. Puedes hacer referencia a respuestas anteriores si es apropiado." : ""}

FORMATO:
- Usa **negrita** para t√©rminos importantes
- Usa *it√°lica* para √©nfasis
- Usa listas con - o n√∫meros cuando enumeres cosas
- Usa \`c√≥digo\` para comandos o c√≥digo
- Usa bloques de c√≥digo con \`\`\` para ejemplos de c√≥digo m√°s largos
- Usa ### para subt√≠tulos si es necesario

Mant√©n la respuesta concisa pero completa.`
    : `${systemContext}
${conversationContext}

Solicitud actual del usuario: ${prompt}

---

Responde de manera clara y did√°ctica para personas aprendiendo a programar con IA.`

  let text
  try {
    if (isGemini) {
      // Use Gemini API
      const result = await client.generateContent(fullPrompt)
      const response = await result.response
      text = response.text()
    } else {
      // Use OpenAI API
      const completion = await client.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
          { role: "system", content: systemContext },
          { role: "user", content: fullPrompt }
        ],
        temperature: 1,
        max_completion_tokens: 2000,
      })
      text = completion.choices[0]?.message?.content || "No se pudo generar una respuesta."
    }
  } catch (apiError: any) {
    console.error(`üí• [TUTOR] API call error:`, {
      name: apiError?.name,
      message: apiError?.message,
      status: apiError?.status,
      statusCode: apiError?.statusCode
    })
    throw apiError // Re-throw to be caught by outer try-catch
  }
  
  console.log(`üì§ [TUTOR] Returning response (${text.length} chars)`)

  return NextResponse.json({ 
    response: text,
    success: true,
    provider,
    fallbackUsed: usedFallback
  })
}

// Handle Br√∫jula mode - AI-powered navigation
async function handleBrujulaMode(query: string, client: any, provider: string, usedFallback: boolean = false, isGemini: boolean = false) {
  const apiType = usedFallback ? 'FALLBACK' : 'PRIMARY'
  console.log(`‚öôÔ∏è  [BR√öJULA] Processing with ${apiType} API (${provider})`)
  
  if (!query || query.trim().length === 0) {
    return NextResponse.json(
      { error: "La consulta es requerida", success: false },
      { status: 400 }
    )
  }

  const contentKnowledge = await formatContentKnowledgeForGemini()
  console.log(`üìö [BR√öJULA] Content knowledge loaded (${contentKnowledge.length} chars)`)

  const brujulaPrompt = `${contentKnowledge}

---

CONSULTA DEL USUARIO: "${query}"

---

INSTRUCCIONES:
Eres un asistente emp√°tico que ayuda a vibecoders (personas aprendiendo a programar con IA) a encontrar TODO el contenido que necesitan para tener √©xito.

MENTALIDAD CLAVE:
- Los vibecoders son principiantes que no saben lo que no saben
- Si preguntan por una herramienta, probablemente necesitan conocer TODAS las herramientas relacionadas
- Si tienen un problema, necesitan la cadena completa de conocimiento para resolverlo
- Piensa en el "journey" completo del usuario, no solo en la respuesta literal

EJEMPLOS DE PENSAMIENTO EMP√ÅTICO:
- Pregunta: "¬øQu√© es una consola?" ‚Üí Necesita: Terminal, comandos b√°sicos, Y la gu√≠a de devtools (porque va a encontrar errores ah√≠)
- Pregunta: "C√≥mo uso Cursor?" ‚Üí Necesita: Gu√≠a de Cursor, Terminal (lo va a usar), DevTools (va a debuggear), Git (va a versionar)
- Pregunta: "C√≥mo hago un buen prompt?" ‚Üí Necesita: Heur√≠sticas (OBLIGATORIO), glosario de IA (temperatura, contexto, etc), gu√≠as de vibecoding
- Pregunta: "C√≥mo usar bien la IA?" ‚Üí Necesita: Heur√≠sticas (OBLIGATORIO - es la p√°gina dedicada a esto), conceptos de IA, gu√≠as de vibecoding
- Pregunta: "D√≥nde veo errores?" ‚Üí Necesita: DevTools, Terminal, Y c√≥mo copiar errores para v0/Claude

REGLA ESPECIAL: Si la pregunta es sobre "usar IA", "mejores prompts", "comunicarse con IA", "instrucciones a la IA" o similar, la p√°gina de Heur√≠sticas y Buenas Pr√°cticas es OBLIGATORIA como primer link.

REGLA DE ORO: Incluye 3-5 links que cubran el objetivo impl√≠cito completo, no solo la pregunta literal.

1. Analiza la consulta y detecta:
   - ¬øQu√© quiere hacer el usuario? (objetivo expl√≠cito)
   - ¬øQu√© va a necesitar para lograrlo? (objetivo impl√≠cito)
   - ¬øQu√© herramientas/conocimientos complementarios son necesarios?

2. Responde en formato JSON estrictamente con esta estructura:
{
  "answer": "Respuesta amigable y clara explicando qu√© contenido le puede servir al usuario (2-3 oraciones m√°ximo). Usa un tono cercano y did√°ctico.",
  "links": [
    {
      "title": "T√≠tulo del contenido",
      "url": "/ruta/completa#hash-si-aplica",
      "description": "Breve descripci√≥n de por qu√© este link es relevante"
    }
  ],
  "fallback": false
}

3. REGLAS IMPORTANTES:
   - El campo "answer" debe ser texto plano, SIN markdown ni formato especial
   - Incluye 3-5 links que cubran el journey completo del usuario
   - Prioriza gu√≠as y p√°ginas de contenido sobre t√©rminos individuales cuando sea apropiado
   - Las URLs deben ser exactas seg√∫n el mapa de contenido
   - Si es un t√©rmino de glosario, la URL debe incluir el hash (ej: /dashboard/glossary/development#git)
   - Si NO hay contenido relevante en el mapa, usa "fallback": true y sugiere alternativas (ChatGPT, grupo WhatsApp)
   - S√© conciso pero completo en la respuesta

AHORA RESPONDE A LA CONSULTA DEL USUARIO en formato JSON puro (sin markdown, sin bloques de c√≥digo, solo el objeto JSON).`

  let text
  try {
    if (isGemini) {
      // Use Gemini API
      const result = await client.generateContent(brujulaPrompt)
      const response = await result.response
      text = response.text()
    } else {
      // Use OpenAI API
      const completion = await client.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
          { role: "system", content: "Eres un asistente experto en navegaci√≥n de contenido educativo." },
          { role: "user", content: brujulaPrompt }
        ],
        temperature: 1,
        max_completion_tokens: 1500,
        response_format: { type: "json_object" }
      })
      text = completion.choices[0]?.message?.content || "{}"
    }
  } catch (apiError: any) {
    console.error(`üí• [BR√öJULA] API call error:`, {
      name: apiError?.name,
      message: apiError?.message,
      status: apiError?.status,
      statusCode: apiError?.statusCode
    })
    throw apiError // Re-throw to be caught by outer try-catch
  }

  // Clean up response - remove markdown code blocks if present
  text = text.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim()

  // Parse JSON response
  let brujulaResponse
  try {
    brujulaResponse = JSON.parse(text)
    console.log(`‚ú® [BR√öJULA] Successfully parsed response with ${brujulaResponse?.links?.length || 0} links`)
  } catch (parseError) {
    console.error("‚ùå [BR√öJULA] Failed to parse OpenAI JSON response:", text?.substring(0, 200))
    return NextResponse.json({
      error: "Error al procesar la respuesta de b√∫squeda",
      success: false 
    }, { status: 500 })
  }

  console.log(`üì§ [BR√öJULA] Returning response`)
  return NextResponse.json({
    brujulaResponse,
    success: true,
    provider,
    fallbackUsed: usedFallback
  })
}
